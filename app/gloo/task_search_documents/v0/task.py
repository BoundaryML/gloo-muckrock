
import typing
from gloo_py import GlooTaskInterface, LLMClient, OpenAILLMClient
from ...types import SearchInputModel, SearchOutputModel
from .generated import search_documents__Description, VARS

class search_documents(GlooLLMTaskInterface):
    """
    This file is autogenerated by gloo.
    Only modify functions that are prefixed with edit_.
    Do not change function signatures for any functions as that may lead to unexpected behavior.
    """

    ## The default name is the class name. 
    ## If you want to change it, uncomment the following function.
    # def edit_name(self) -> str:
    #    return "search_documents"
    
    ## The default client is OpenAILLMClient, gpt-3.5-turbo (latest model), temperature=0.
    ## If you want to change it, uncomment the following function.
    ##
    ## alternatively, you can configure a global default client.
    ## from gloo_py import set_default_llm_client
    ## set_default_llm_client(...)
    # def edit_llm_client(self) -> LLMClient:
    #    return OpenAILLMClient(model_name="gpt-3.5-turbo", temperature=0)

    def edit_prompt_template(self) -> str:
        # Prompts are auto dedented and trimmed at the start. The end of the prompt is not trimmed.
        return f'''
        Write your prompt here. For any variable not related to your input or output model, use {{var_name}} and add the value 
        to prompt_paramters or pass it into the run function. You can use the VARS object to access the input and output models.

        INPUT:
        {VARS.in_SearchInput}

        OUTPUT FORMAT:
        {VARS.out_SearchOutput.json}

        JSON:
        '''

    ## Prompt paramters are variables that you can refer to in your prompt using
    ## {{var_name}}. These are static, and are not passed in at runtime. variables
    ## allow you to segment your prompt into pieces, and query these individual pieces
    ## separately in the analytics dashboard.
    # def edit_prompt_parameters(self) -> typing.Dict[str, str]:
    #    return {
    #        # Custom fields that can be tracked to easily compare their impact.
    #        "var_name": "some_value"
    #    }
   
    ## This function allows you to describe the output format for your data. You can use
    ## this to indicate to the LLM what each field actually means.
    ## A description of each schema field may just be "string[]" or "number" for example, or a more natural language explanation like "the name of the user, as a string".
    ## You can refer to any descriptions you define here in your prompt by using
    ## {VARS.output.var_name.desc}
    def edit_types_json_converter(self) -> search_documents__Description:
        return {
            "SearchOutput": { "clues": { "name": "clues", "description": "string[]", }, "reasoning": { "name": "reasoning", "description": "string", }, "isAnswerInContext": { "name": "isAnswerInContext", "description": "string", }, "answer": { "name": "answer", "description": "string", }, },
        }

    ## By default we assume the model outputs json and we parse it using the pydantic model.
    ## GlooLLMTaskInterface.parse_raw converts aliases to their field names.
    ## Highly recommend calling super().parse() if you override this function.
    # def edit_parser(self, raw_llm_response: str) -> SearchOutput:
    #    return super().parse(SearchOutput, raw_llm_response)

async def run_v0(*, input: SearchInputModel) -> SearchOutputModel:
    task = search_documents()
    # You can add additional parameters to task.run.
    # See docs for what paramters work: DOCS_LINK
    return await task.run(input)

# Only the run_v0 function is directed exported.
__all__ = ["run_v0", "SearchInputModel", "SearchOutputModel"]
