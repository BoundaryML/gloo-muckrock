import typing
from gloo_py import GlooLLMTaskInterface, LLMClient, OpenAILLMClient
from ...types import ClassifyRequestInputModel, ClassifyRequestOutputModel
from ...types import StatusModel, StatusModel__Definition
from ...types import ClassificationModel, ClassificationModel__Definition
from .generated import process_request__Definition, VARS
import asyncio


class process_request(GlooLLMTaskInterface):
    """
    This file is autogenerated by gloo.
    Only modify functions that are prefixed with override_.
    Do not change function signatures for any functions as that may lead to unexpected behavior.
    """

    # The default name is the class name.
    # If you want to change it, uncomment the following function.
    # def override_name(self) -> str:
    #    return "process_request"

    # The default client is OpenAILLMClient, gpt-3.5-turbo (latest model), temperature=0.
    # If you want to change it, uncomment the following function.
    #
    # Alternatively, you can configure a global default client.
    #
    # > from gloo_py import set_default_llm_client, OpenAILLMClient
    # > set_default_llm_client(OpenAILLMClient(model_name="gpt-4", temperature=1))
    #
    # def override_llm_client(self) -> LLMClient:
    #    return OpenAILLMClient(model_name="gpt-3.5-turbo", temperature=0)

    def override_prompt(self) -> str:
        # Prompts are auto dedented and trimmed at the start. The end of the prompt is not trimmed.
        return f"""
        Write your prompt here. For any variable not related to your input or output model, use {{var_name}} and add the value 
        to prompt_paramters or pass it into the run function. You can use the VARS object to access the input and output models.

        INPUT:
        {VARS.input}

        OUTPUT FORMAT:
        {VARS.output}

        JSON:
        """

    # Prompt paramters are variables that you can refer to in your prompt using
    # {{var_name}}. These are static, and are not passed in at runtime. variables
    # allow you to segment your prompt into pieces, and query these individual pieces
    # separately in the analytics dashboard.
    #
    # def override_static_vars(self) -> typing.Dict[str, str]:
    #    return {
    #        # Custom fields that can be tracked to easily compare their impact.
    #        "var_name": "some_value"
    #    }

    # If you wish to format the input in your own manner you can do so here.
    # def override_input_vars(self, input: ClassifyRequestInputModel) -> typing.Dict[str, str]:
    #    return {
    #        # Custom fields that can be tracked to easily compare their impact.
    #        "my_custom_var": input
    #    }

    # This function allows you to describe the output format for your data. You can use
    # this to indicate to the LLM what each field actually means.
    # A definition of each schema field may just be "string[]" or "number" for example, or a more natural language explanation like "the name of the user, as a string".
    # You can refer to any definitions you define here in your prompt by using
    # {VARS.output.trackingNumber.dfn}
    def override_output_definitions(self) -> process_request__Definition:
        return process_request__Definition(
            **{
                "Status": StatusModel__Definition(
                    alias="Status",
                    definition="string",
                    case_name_formatter=lambda name: name,
                    case_formatter=lambda name, desc: f"{name}: {desc}",
                    cases={
                        "PROCESSED": {
                            "alias": "PROCESSED",
                            "definition": "The agency is working on the request and will respond again. This includes, but is not limited to, acknowledgment letters and automated responses from portals or online systems.",
                        },
                        "FIX": {
                            "alias": "FIX",
                            "definition": "The agency has asked the requestor for clarification, to supply additional information, to narrow down a request, or complete an additional task in order to allow them to continue processing the request",
                        },
                        "PAYMENT": {
                            "alias": "PAYMENT",
                            "definition": "The requestor must pay a fee for the agency to continue processing",
                        },
                        "REJECTED": {
                            "alias": "REJECTED",
                            "definition": "The request has been denied",
                        },
                        "NO_DOCS": {
                            "alias": "NO_DOCS",
                            "definition": "The agency has determined that there are no responsive documents for",
                        },
                        "DONE": {
                            "alias": "DONE",
                            "definition": "The agency has supplied or attached all of the responsive documents or records for the request. This is the final response.",
                        },
                        "PARTIAL": {
                            "alias": "PARTIAL",
                            "definition": "The agency has supplied some of the responsive documents, but has indicated that they will be releasing more documents in the future",
                        },
                        "INDETERMINATE": {
                            "alias": "INDETERMINATE",
                            "definition": "Use this status if you cannot determine the correct status from the information provided.",
                        },
                    },
                ),
                "Classification": {
                    "clues": {
                        "alias": "clues",
                        "definition": "2-3 sentences as a string",
                    },
                    "reasoning": {
                        "alias": "reasoning",
                        "definition": "string",
                    },
                    "status": {
                        "alias": "status",
                        "definition": f"{VARS.out_Classification.status.desc}",
                    },
                },
                "ClassifyRequestOutput": {
                    "trackingNumber ": {
                        "alias": "trackingNumber",
                        "definition": "string",
                    },
                    "dateEstimate": {
                        "alias": "dateEstimate",
                        "definition": "string",
                    },
                    "price": {
                        "alias": "price",
                        "definition": "int",
                    },
                    "classification": {
                        "alias": "classification",
                        "definition": "Status",
                    },
                    "reasoning": {
                        "alias": "reasoning",
                        "definition": "string",
                    },
                },
            }
        )

    # By default we assume the model outputs json and we parse it using the pydantic model.
    # GlooLLMTaskInterface.parse_raw converts aliases to their field names.
    # Highly recommend calling super().parse() if you override this function.
    # def override_parser(self, model: typing.Type[ClassifyRequestOutputModel], raw_llm_response: str) -> ClassifyRequestOutputModel:
    #    return super().parse(model, raw_llm_response)


# You can add additional parameters to task.run.
# See docs for what paramters work: DOCS_LINK
async def run_process_request_v1_async(
    _in: ClassifyRequestInputModel, **kwargs
) -> ClassifyRequestOutputModel:
    task = process_request()
    return await task.run(_in, output_model=ClassifyRequestOutputModel, **kwargs)


def run_process_request_v1_sync(
    _in: ClassifyRequestInputModel, **kwargs
) -> ClassifyRequestOutputModel:
    return asyncio.run(run_process_request_v1_async(_in, **kwargs))


# Only the run_v1 function is directed exported.
__all__ = [
    "run_process_request_v1_async",
    "run_process_request_v1_sync",
    "ClassifyRequestInputModel",
    "ClassifyRequestOutputModel",
]
